{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering de News : K-Means sur SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n"
     ]
    }
   ],
   "source": [
    "### Il y a quelques \"perles\" Python   qui facilitent les choses !!!\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Barre de progrès des calculs\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# Arguments éventuels de la ligne de commande\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    " \n",
    "# Sous  Jupyter notebook enlever ces lignes\n",
    "je_suis_dans_jupyter=True\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "\n",
    "if not je_suis_dans_jupyter :\n",
    "    #argv = [] if is_interactive() else sys.argv[1:]\n",
    "    #(opts, args) = op.parse_args(argv)\n",
    "    if len(args) > 0:\n",
    "        op.error(\"this script takes no arguments.\")\n",
    "        sys.exit(1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace :\n",
    "    Automatically created module for IPython interactive environment\n",
    "    Usage: ipykernel_launcher.py [options]\n",
    "\n",
    "    Options:\n",
    "      -h, --help            show this help message and exit\n",
    "      --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
    "      --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
    "      --no-idf              Disable Inverse Document Frequency feature weighting.\n",
    "      --use-hashing         Use a hashing feature vectorizer\n",
    "      --n-features=N_FEATURES\n",
    "                            Maximum number of features (dimensions) to extract\n",
    "                            from text.\n",
    "      --verbose             Print progress reports inside k-means algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387 documents\n",
      "4 categories\n",
      "\n",
      "Extraction des features du training set avec vectorisation\n",
      "Fait en  0.661316s\n",
      "Info : nb samples: 3387, nb features: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Chageons QQ catégories depuis l'ensemble d'apprentissage\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Si décommenté, on fera Toutes les catégoroes !\n",
    "# categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "# Voir les imports\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"Extraction des features du training set avec vectorisation\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', alternate_sign=False,\n",
    "                                   norm=None, binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"Fait en  %fs\" % (time() - t0))\n",
    "print(\"Info : nb samples: %d, nb features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "    print(\"SVD LSA\", end=' ')\n",
    "    print(\"de \",  X.shape[1], \" features à\", opts.n_components, \" features\")\n",
    "    t0 = time()\n",
    "    # Les résultats sont déjà normalisés. Ce qui fait que KMeans se comportera comme \n",
    "    # spherical k-means (mieux). \n",
    "    # ATTENTION : les résultats de  LSA/SVD ne sont pas Re-normalisés\n",
    "    # --->> ON DOIT LE REFAIRE\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"Fait en %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Variance expliquée par la SVD : {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace\n",
    "    Loading 20 newsgroups dataset for categories:\n",
    "    ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "    3387 documents\n",
    "    4 categories\n",
    "\n",
    "    Extraction des features du training set avec vectorisation\n",
    "    Fait en  0.675751s\n",
    "    Info : nb samples: 3387, nb features: 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnes creuse de Clustering avec MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "                init_size=1000, max_iter=100, max_no_improvement=10,\n",
      "                n_clusters=4, n_init=1, random_state=None,\n",
      "                reassignment_ratio=0.01, tol=0.0, verbose=False)\n",
      "Fait en 0.126s\n",
      "\n",
      "Homogeneity: 0.595\n",
      "Completeness: 0.604\n",
      "V-measure: 0.599\n",
      " Rand-Index ajusté : 0.610\n",
      "Coefficient Silhouette : 0.007\n",
      "\n",
      "Meilleurs termes par cluster:\n",
      "Cluster 0: graphics university image thanks com file files 3d ac posting\n",
      "Cluster 1: god sandvik jesus kent com bible apple christian people newton\n",
      "Cluster 2: space nasa henry access digex gov toronto pat alaska shuttle\n",
      "Cluster 3: com keith sgi livesey article people caltech don morality think\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# clustering\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Donnes creuse de Clustering avec %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"Fait en %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Quelques mesures propres au clustering et Kmeans\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\" Rand-Index ajusté : %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Coefficient Silhouette : %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Meilleurs termes par cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace\n",
    "    Donnes creuse de Clustering avec MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
    "                    init_size=1000, max_iter=100, max_no_improvement=10,\n",
    "                    n_clusters=4, n_init=1, random_state=None,\n",
    "                    reassignment_ratio=0.01, tol=0.0, verbose=False)\n",
    "    Fait en 0.073s\n",
    "\n",
    "    Homogeneity: 0.491\n",
    "    Completeness: 0.524\n",
    "    V-measure: 0.507\n",
    "     Rand-Index ajusté : 0.413\n",
    "    Coefficient Silhouette : 0.008\n",
    "\n",
    "    Meilleurs termes par cluster:\n",
    "    Cluster 0: god com people sandvik keith don article jesus say think\n",
    "    Cluster 1: image images color polygon thanks bit 24 university vga graphics\n",
    "    Cluster 2: graphics com university posting host nntp 3d ac computer ca\n",
    "    Cluster 3: space nasa access henry digex pat toronto gov alaska moon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
